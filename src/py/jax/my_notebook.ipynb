{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm as tq\n",
    "import gzip, pickle\n",
    "import jax\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"100k_csv/ratings.csv\").sort_values(by='userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test users in train: True\n",
      "All test movies in train: True\n",
      "Number of users in Train: 610\n",
      "Number of users in Test: 345\n",
      "Number of movies in Train: 9724\n",
      "Number of movies in Test: 755\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.01, random_state=42)\n",
    "\n",
    "# Get unique users and movies in train and test sets\n",
    "users_in_train = train_df['userId'].unique()\n",
    "users_in_test = test_df['userId'].unique()\n",
    "movies_in_train = train_df['movieId'].unique()\n",
    "movies_in_test = test_df['movieId'].unique()\n",
    "\n",
    "# Convert to sets for efficient comparison\n",
    "train_users_set = set(users_in_train)\n",
    "test_users_set = set(users_in_test)\n",
    "train_movies_set = set(movies_in_train)\n",
    "test_movies_set = set(movies_in_test)\n",
    "\n",
    "# Find users and movies to move from test to train\n",
    "users_to_move = test_users_set - train_users_set  # Users only in the test set\n",
    "movies_to_move = test_movies_set - train_movies_set  # Movies only in the test set\n",
    "\n",
    "# Move users from test to train\n",
    "if users_to_move:\n",
    "    user_data_to_move = test_df[test_df['userId'].isin(users_to_move)]\n",
    "    train_df = pd.concat([train_df, user_data_to_move])  # Move these users to train\n",
    "    test_df = test_df[~test_df['userId'].isin(users_to_move)]  # Remove from test\n",
    "\n",
    "# Move movies from test to train\n",
    "if movies_to_move:\n",
    "    movie_data_to_move = test_df[test_df['movieId'].isin(movies_to_move)]\n",
    "    train_df = pd.concat([train_df, movie_data_to_move])  # Move these movies to train\n",
    "    test_df = test_df[~test_df['movieId'].isin(movies_to_move)]  # Remove from test\n",
    "\n",
    "# Get unique users and movies in train and test sets\n",
    "users_in_train = train_df['userId'].unique()\n",
    "users_in_test = test_df['userId'].unique()\n",
    "movies_in_train = train_df['movieId'].unique()\n",
    "movies_in_test = test_df['movieId'].unique()\n",
    "\n",
    "# Convert to sets for efficient comparison\n",
    "train_users_set = set(users_in_train)\n",
    "test_users_set = set(users_in_test)\n",
    "train_movies_set = set(movies_in_train)\n",
    "test_movies_set = set(movies_in_test)\n",
    "\n",
    "# Verify that all test users and movies are now in train using set intersection\n",
    "all_test_users_in_train = test_users_set.issubset(train_users_set)\n",
    "all_test_movies_in_train = test_movies_set.issubset(train_movies_set)\n",
    "\n",
    "# Output the result\n",
    "print(f\"All test users in train: {all_test_users_in_train}\")\n",
    "print(f\"All test movies in train: {all_test_movies_in_train}\")\n",
    "\n",
    "# Check the result for the number of unique users and movies\n",
    "print(f\"Number of users in Train: {train_df['userId'].nunique()}\")\n",
    "print(f\"Number of users in Test: {test_df['userId'].nunique()}\")\n",
    "print(f\"Number of movies in Train: {train_df['movieId'].nunique()}\")\n",
    "print(f\"Number of movies in Test: {test_df['movieId'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99866, 970, True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(test_df), len(train_df) + len(test_df) == len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_users_into_chunks(df, num_chunks):\n",
    "    \"\"\"\n",
    "    Split a DataFrame into chunks based on unique users, \n",
    "    with users shuffled in each call.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing user ratings.\n",
    "    num_chunks (int): The number of desired DataFrames (chunks).\n",
    "\n",
    "    Returns:\n",
    "    list: A list of lists, each containing indices of rows for a unique group of users.\n",
    "    \"\"\"\n",
    "    # Step 1: Get unique users and shuffle them\n",
    "    unique_users = df['userId'].unique()\n",
    "    np.random.shuffle(unique_users)\n",
    "\n",
    "    # Step 2: Split users into groups\n",
    "    user_groups = np.array_split(unique_users, num_chunks)\n",
    "\n",
    "    # Step 3: Create a list of indices based on user groups\n",
    "    index_groups = []\n",
    "    for group in user_groups:\n",
    "        # Get the indices of the rows corresponding to the current group of users\n",
    "        indices = df[df['userId'].isin(group)].index.tolist()\n",
    "        \n",
    "        index_groups.append(indices)\n",
    "\n",
    "    return index_groups, user_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "# Objective Function: J(U, V)\n",
    "@jax.jit\n",
    "def loss(U, V, b_u, b_i, mu, R, lam):\n",
    "    # Compute the predicted rating matrix including biases\n",
    "    R_hat = mu + b_u[:, None] + b_i[None, :] + jnp.dot(U, V.T)\n",
    "    \n",
    "    # Compute the error matrix\n",
    "    E = R - R_hat\n",
    "    E = jnp.where(R == 0, 0, E)\n",
    "    \n",
    "    # Calculate the loss (mean squared error + regularization)\n",
    "    squared_error = jnp.sum(E**2)/jnp.sum(R > 0)\n",
    "    regularization = lam * (jnp.sum(U**2) + jnp.sum(V**2) + jnp.sum(b_u**2) + jnp.sum(b_i**2))\n",
    "    \n",
    "    # Return the total objective value\n",
    "    return squared_error + regularization\n",
    "\n",
    "# Gradients with respect to U, V, b_u, and b_i using JAX\n",
    "grad_U = jax.jit(grad(loss, argnums=0))  # Gradient with respect to U\n",
    "grad_V = jax.jit(grad(loss, argnums=1))  # Gradient with respect to V\n",
    "grad_b_u = jax.jit(grad(loss, argnums=2))  # Gradient with respect to user biases\n",
    "grad_b_i = jax.jit(grad(loss, argnums=3))  # Gradient with respect to item biases\n",
    "\n",
    "# SGD update step\n",
    "@jax.jit\n",
    "def sgd_step(U, V, b_u, b_i, mu, R, lam, alpha):\n",
    "    dU = grad_U(U, V, b_u, b_i, mu, R, lam)  # Gradient wrt U\n",
    "    dV = grad_V(U, V, b_u, b_i, mu, R, lam)  # Gradient wrt V\n",
    "    db_u = grad_b_u(U, V, b_u, b_i, mu, R, lam)  # Gradient wrt user biases\n",
    "    db_i = grad_b_i(U, V, b_u, b_i, mu, R, lam)  # Gradient wrt item biases\n",
    "    \n",
    "    # Update the parameters U, V, b_u, and b_i\n",
    "    U_new = U - alpha * dU\n",
    "    V_new = V - alpha * dV\n",
    "    b_u_new = b_u - alpha * db_u\n",
    "    b_i_new = b_i - alpha * db_i\n",
    "    \n",
    "    return U_new, V_new, b_u_new, b_i_new\n",
    "\n",
    "def matrix_factorization_sgd(weights, R, mu, lam, alpha, iterations):\n",
    "    U, V, b_u, b_i = weights\n",
    "    \n",
    "    def body(carry, _):\n",
    "        U, V, b_u, b_i = carry\n",
    "        # Perform the SGD update\n",
    "        U, V, b_u, b_i = sgd_step(U, V, b_u, b_i, mu, R, lam, alpha)\n",
    "        # Return the updated values and the carry\n",
    "        return (U, V, b_u, b_i), None\n",
    "    \n",
    "    carry = (U, V, b_u, b_i)\n",
    "    \n",
    "    # Use lax.scan to perform the updates\n",
    "    final_carry, _ = jax.lax.scan(body, carry, jnp.arange(iterations))\n",
    "    \n",
    "    # Final carry will contain the updated U, V, b_u, b_i\n",
    "    return final_carry\n",
    "\n",
    "def train(df, num_chunks):\n",
    "    # Parameters\n",
    "    K = 10  # Number of latent factors\n",
    "    lam = 0.1  # Regularization strength\n",
    "    alpha = 0.1  # Learning rate\n",
    "    iterations = 1000  # Number of iterations\n",
    "    number_of_epochs = 5\n",
    "    num_users, num_items = df.userId.nunique(), df.movieId.nunique()\n",
    "    key = jax.random.PRNGKey(seed=42)\n",
    "    u_key, v_key, b_u_key, b_i_key = jax.random.split(key, 4)\n",
    "    \n",
    "    jit_loss = jax.jit(loss)\n",
    "    U = jax.random.normal(u_key, shape=(num_users, K)) * 0.01\n",
    "    V = jax.random.normal(v_key, shape=(num_items, K)) * 0.01\n",
    "    b_u = jnp.zeros(num_users)  # Initialize user biases\n",
    "    b_i = jnp.zeros(num_items)  # Initialize item biases\n",
    "    mu = df['rating'].mean()\n",
    "    \n",
    "    for epoch in range(number_of_epochs):\n",
    "        total_loss = 0\n",
    "        index_groups, user_groups = split_users_into_chunks(df, num_chunks=num_chunks)\n",
    "        for i, (indices, user) in enumerate(zip(index_groups, user_groups)):\n",
    "            # Pivot the DataFrame to create the user-movie matrix\n",
    "            R = df.loc[indices].pivot(index='userId', columns='movieId', values='rating').fillna(0).to_numpy()\n",
    "            movie = df.loc[indices].movieId.unique()\n",
    "            weights = U[user], V[movie], b_u[user], b_i[movie]\n",
    "            \n",
    "            # Perform SGD step and update weights\n",
    "            U_, V_, b_u_, b_i_ = matrix_factorization_sgd(weights, R, mu, lam, alpha, iterations)\n",
    "            U = U.at[user].set(U_)  # Update U at the index 'user'\n",
    "            b_u = b_u.at[user].set(b_u_)  # Update b_u at the index 'user'\n",
    "            V = V.at[movie].set(V_)\n",
    "            b_i = b_i.at[movie].set(b_i_)\n",
    "            # Calculate the loss for this chunk and accumulate it\n",
    "            current_loss = loss(U_, V_, b_u_, b_i_, mu, R, lam)\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print(f\"Steps {(epoch + 1) * (i + 1) * iterations}, Loss: {current_loss}\")\n",
    "        \n",
    "    \n",
    "    return U, V, b_u, b_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 50, Loss: 1.3080358505249023\n",
      "Steps 550, Loss: 0.6956747174263\n",
      "Steps 1050, Loss: 1.148828148841858\n",
      "Steps 1550, Loss: 0.7033240795135498\n",
      "Steps 2050, Loss: 1.100050687789917\n",
      "Steps 100, Loss: 1.031233549118042\n",
      "Steps 1100, Loss: 0.8425652980804443\n",
      "Steps 2100, Loss: 0.7444784641265869\n",
      "Steps 3100, Loss: 1.196338176727295\n",
      "Steps 4100, Loss: 1.2229278087615967\n",
      "Steps 150, Loss: 1.2250889539718628\n",
      "Steps 1650, Loss: 1.141504168510437\n",
      "Steps 3150, Loss: 0.838412880897522\n",
      "Steps 4650, Loss: 0.8361217975616455\n",
      "Steps 6150, Loss: 1.1083306074142456\n",
      "Steps 200, Loss: 1.1075458526611328\n",
      "Steps 2200, Loss: 1.2313172817230225\n"
     ]
    }
   ],
   "source": [
    "del df\n",
    "\n",
    "U, V, b_u, b_i = train(train_df, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "970it [00:01, 713.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.0451021194458008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_rmse(U, V, b_u, b_i, mu, test_df):\n",
    "    # Initialize variables to accumulate the total squared error and count of predictions\n",
    "    total_squared_error = 0.0\n",
    "    count = 0\n",
    "    \n",
    "    # Iterate through each row in the test DataFrame\n",
    "    for _, row in tq(test_df.iterrows()):\n",
    "        user_id = int(row['userId'])\n",
    "        movie_id = int(row['movieId'])\n",
    "        true_rating = row['rating']\n",
    "        \n",
    "        # Make prediction using the matrix factorization model\n",
    "        predicted_rating = mu + b_u[user_id] + b_i[movie_id] + jnp.dot(U[user_id], V[movie_id])\n",
    "        \n",
    "        # Calculate the squared error for this prediction\n",
    "        squared_error = (predicted_rating - true_rating) ** 2\n",
    "        \n",
    "        # Accumulate the squared error and count the predictions\n",
    "        total_squared_error += squared_error\n",
    "        count += 1\n",
    "    \n",
    "    # Calculate the Mean Squared Error (MSE)\n",
    "    mse = total_squared_error / count\n",
    "    \n",
    "    # Return the Root Mean Square Error (RMSE)\n",
    "    return jnp.sqrt(mse)\n",
    "\n",
    "mu = train_df['rating'].mean()\n",
    "rmse_value = compute_rmse(U, V, b_u, b_i, mu, test_df[:10000])\n",
    "print(f\"RMSE: {rmse_value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mavaenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
